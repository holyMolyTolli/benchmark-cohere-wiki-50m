import glob
import json
import os
import re
from datetime import datetime, timedelta

import numpy as np
import pandas as pd

# ==========================================
# ‚öôÔ∏è CONFIGURATION - EDIT THESE TO MATCH YOUR FILES
# ==========================================
# The CSV generated by the Benchmark script
BASE_OUTPUT_DIR = os.getenv("BASE_OUTPUT_DIR")
BASE_OUTPUT_DIR = "benchmark_results_rw_06"
BENCHMARK_FOLDER = f"{BASE_OUTPUT_DIR}/raw_benchmark_data"


TOTAL_CLUSTER_VCPUS = 64  # 4 nodes * 16 vCPU
TOTAL_CLUSTER_RAM_GB = 512  # 4 nodes * 128 GB (Adjust if different)
SERVER_NODE_COUNT = 4
AVAILABLE_CLIENT_MEMORY_MB = 32768
TOTAL_DISK_CAPACITY = 1024 * 1024 * 1024 * 1024  # 1024GiB
# ==========================================
# üõ†Ô∏è PARSING FUNCTIONS
# ==========================================


def parse_prometheus_data(filepath):
    metrics = {}

    pattern = re.compile(r"^([a-zA-Z0-9_]+)(?:\{(.*)\})?\s+(.+)$")

    with open(filepath, "r") as f:
        contents = f.readlines()

    for line in contents:
        line = line.strip()
        # Skip comments and empty lines
        if not line or line.startswith("#"):
            continue

        match = pattern.match(line)
        if match:
            name = match.group(1)
            label_str = match.group(2)
            value_str = match.group(3)

            # Parse labels into a dict
            labels = {}
            if label_str:
                # Split by comma, but careful with quoted strings containing commas
                # Simple split usually works for standard prom output
                pairs = re.findall(r'([a-zA-Z0-9_]+)="([^"]*)"', label_str)
                labels = {k: v for k, v in pairs}

            try:
                # Handle "Inf" and standard floats
                if "Inf" in value_str:
                    val = float("inf")
                else:
                    val = float(value_str)
            except ValueError:
                continue

            if name not in metrics:
                metrics[name] = []

            metrics[name].append({"labels": labels, "value": val})

    cleaned_metrics = {}
    for key, value in metrics.items():
        value_sum = 0
        for item in value:
            pod = item["labels"].get("pod")
            persistentvolumeclaim = item["labels"].get("persistentvolumeclaim")
            if pod:
                use = pod
            elif persistentvolumeclaim:
                use = persistentvolumeclaim
            else:
                print(f"No pod or persistentvolumeclaim found for {key}")
                continue
            pod_id = use.split("-")[-1]
            new_key = key + "_" + str(pod_id)
            cleaned_metrics[new_key] = item["value"]
            value_sum += item["value"]

        # ----------------------
        # Define prefixes for configuration parameters that should be uniform
        # across nodes and therefore averaged (e.g., vector size, thread limits).
        config_prefixes = "qdrant_collection_config_"

        # Define exact keys that represent pre-calculated averages, timestamps,
        # or logical cluster states that should be averaged.
        keys_to_average_explicit = [
            "collections_total",  # Corrected based on your feedback
            "rest_responses_avg_duration_seconds",
            "rest_responses_max_duration_seconds",
            "rest_responses_min_duration_seconds",
            "kube_pod_status_ready_time",
            # Add any other specific pre-calculated rates/ratios here
        ]

        # Check if the key matches config prefixes or explicit average keys
        should_average = key.startswith(config_prefixes) or key in keys_to_average_explicit

        if should_average:
            # Average: for configs, rates, and logical counts
            cleaned_metrics[key] = value_sum / SERVER_NODE_COUNT
        else:
            # Default to SUM: for counters, capacities, bytes, CPU seconds, and histogram buckets
            cleaned_metrics[key] = value_sum

    return cleaned_metrics


def parse_telemetry_latest(folder_path):
    files = sorted(glob.glob(os.path.join(folder_path, "telemetry", "*.json")))

    total_vectors = 0
    for f in files:
        with open(f, "r") as f:
            data = json.load(f)
            shards = data["result"]["collections"]["collections"][0]["shards"]
            for shard in shards:
                try:
                    total_vectors += shard["local"]["num_vectors"]
                except:
                    continue

    return total_vectors


def compute_benchmark_metrics(file_list):
    """
    Parses a list of Qdrant benchmark JSON files and computes the exact same
    metrics (Min, Avg, Median, P95, P99, Max) as the Rust implementation.
    """

    # Internal helper to match Rust's 'print_stats' logic
    def calc_stats(values, include_percentiles=True):
        if not values:
            return None

        # 1. Sort values (Rust: values.sort_unstable_by)
        values.sort()
        count = len(values)

        # 2. Compute Basic Metrics
        # Rust: values.iter().sum::<f64>() / values.len() as f64
        avg_val = sum(values) / count
        # Rust: values.first()
        min_val = values[0]
        # Rust: values.last()
        max_val = values[-1]

        # Rust: values[(values.len() as f32 * 0.50) as usize]
        # Python int() behaves like Rust cast (truncates decimals)
        p50_idx = int(count * 0.50)
        # Safety clamp not needed for 0.5 unless list is empty, but good practice
        p50_idx = min(p50_idx, count - 1)
        median_val = values[p50_idx]

        stats = {"min": min_val, "avg": avg_val, "median": median_val, "max": max_val}

        if include_percentiles:
            # Rust: values[(values.len() as f32 * 0.95) as usize]
            p95_idx = int(count * 0.95)
            p95_idx = min(p95_idx, count - 1)
            stats["p95"] = values[p95_idx]

            # Rust Loop for P9s (digits=2 for p99)
            # factor = 1.0 - 0.1^2 = 0.99
            # index = ((values.len() as f64 * factor) as usize).min(values.len() - 1)
            p99_idx = int(count * 0.99)
            p99_idx = min(p99_idx, count - 1)
            stats["p99"] = values[p99_idx]

        return stats

    results = []

    for filepath in file_list:
        # extract start and end time from file e.g. "run_P16_T4_EF16_start20251223_085050_end20251223_085057.json"
        start_time = datetime.strptime("_".join(os.path.basename(filepath).split("_start")[1].split("_")[:2]).replace(".json", ""), "%Y%m%d_%H%M%S")
        end_time = datetime.strptime("_".join(os.path.basename(filepath).split("_end")[1].split("_")[:2]).replace(".json", ""), "%Y%m%d_%H%M%S")

        # --- 1. Extract Settings from Filename ---
        filename = os.path.basename(filepath)
        # 'run_Opt"auto"_Idx1_Seg2000000_SegNum200_IndTh200000_P8_T2_EF32_start20260106_115743_end20260106_115900.json'
        # filename = filename.replace("\"auto\"", "auto")
        match = re.search(r"run_Opt(\"auto\"|\d+)_Idx(\d+)_Seg(\d+)_SegNum(\d+)_IndTh(\d+)_P(\d+)_T(\d+)_EF(\d+)", filename)

        if match:
            max_optimization_threads, max_indexing_threads, max_segment_size, default_segment_number, indexing_threshold, parallel, threads, hnsw_ef = match.groups()
        else:
            max_optimization_threads, max_indexing_threads, max_segment_size, default_segment_number, indexing_threshold, parallel, threads, hnsw_ef = (None, None, None, None, None, None, None, None)

        # --- 2. Load JSON Data ---
        with open(filepath, "r") as f:
            data = json.load(f)

        # The Rust struct 'SearcherResults' serializes these keys:
        # server_timings, rps, full_timings
        server_timings_raw = data.get("server_timings", [])
        full_timings_raw = data.get("full_timings", [])
        rps_raw = data.get("rps", [])

        # --- 3. Compute Metrics (Exactly mapping Rust main function calls) ---

        # Call 1: "Search timings" (full_timings) -> percentiles=True
        search_stats = calc_stats(full_timings_raw, include_percentiles=True)

        # Call 2: "Server timings" (server_timings) -> percentiles=True
        server_stats = calc_stats(server_timings_raw, include_percentiles=True)

        # Call 3: "RPS" (rps) -> percentiles=False
        rps_stats = calc_stats(rps_raw, include_percentiles=False)

        # --- 4. Structure Output ---
        entry = {"filename": filename, "start_time": start_time, "end_time": end_time, "max_optimization_threads": max_optimization_threads, "max_indexing_threads": max_indexing_threads, "max_segment_size": max_segment_size, "default_segment_number": default_segment_number, "indexing_threshold": indexing_threshold, "parallel": parallel, "threads": threads, "hnsw_ef": hnsw_ef, **{"search_" + k: v for k, v in search_stats.items()}, **{"server_" + k: v for k, v in server_stats.items()}, **{"rps_" + k: v for k, v in rps_stats.items()}}
        results.append(entry)

    return results


def find_experiment_for_timestamp(timestamp):
    add_seconds = 0
    mask = (benchmark_df["start_time"] <= timestamp + timedelta(seconds=add_seconds)) & (benchmark_df["end_time"] >= timestamp - timedelta(seconds=add_seconds))
    files = benchmark_df[mask]["filename"].values.copy()
    if len(files) == 1:
        return files[0]
    elif len(files) > 1:
        return files[0]  # or handle differently if multiple matches exist
    else:
        return None


# ==========================================
# üìä EXECUTION
# ==========================================

# get benchmark data
files = glob.glob(os.path.join(BENCHMARK_FOLDER, "*.json"))
benchmark_metrics = compute_benchmark_metrics(files)
benchmark_df = pd.DataFrame(benchmark_metrics)
benchmark_df = benchmark_df.sort_values(by=["parallel", "hnsw_ef", "threads", "max_optimization_threads", "max_indexing_threads", "max_segment_size", "default_segment_number", "indexing_threshold"])


# get prometheus data
sys_files = sorted(glob.glob(os.path.join(BASE_OUTPUT_DIR, "raw_sys_metrics", "*.txt")))
prometheus_datas = []
for sys_file in sys_files:
    prometheus_data = parse_prometheus_data(sys_file)
    ts = "_".join(os.path.basename(sys_file).split("_")[-2:]).replace(".txt", "")
    prometheus_data["timestamp"] = datetime.strptime(ts, "%Y%m%d_%H%M%S")
    mem_limit = prometheus_data.get("kube_pod_container_resource_limits")
    prometheus_data["server_memory_%"] = prometheus_data["container_memory_working_set_bytes"] / mem_limit * 100
    disc_limit = prometheus_data.get("kubelet_volume_stats_capacity_bytes")
    prometheus_data["server_disk_%"] = prometheus_data.get("kubelet_volume_stats_used_bytes") / disc_limit * 100
    prometheus_data["server_ram_gb"] = prometheus_data["container_memory_working_set_bytes"] / (1024**3)
    prometheus_data["server_ram_%"] = (prometheus_data["server_ram_gb"] / 256.0) * 100
    prometheus_datas.append(prometheus_data)
prometheus_df = pd.DataFrame(prometheus_datas)
prometheus_df = prometheus_df.sort_values(by="timestamp").reset_index(drop=True)
time_delta_seconds = (prometheus_df["timestamp"] - prometheus_df["timestamp"].shift(1)).dt.total_seconds()
cpu_usage_delta = prometheus_df["container_cpu_usage_seconds_total"] - prometheus_df["container_cpu_usage_seconds_total"].shift(1)
cpu_rate_cores = np.where(time_delta_seconds > 0, cpu_usage_delta / time_delta_seconds, 0.0)
raw_limit_nanocores = prometheus_df.get("kube_pod_container_resource_limits", 0)
cpu_limit_standard_cores = raw_limit_nanocores / 1e9
prometheus_df["server_cpu_%"] = np.where(cpu_limit_standard_cores > 0, (cpu_rate_cores / cpu_limit_standard_cores) * 100, 0.0)


# get client stats data
client_stats_df = pd.read_csv(os.path.join(BASE_OUTPUT_DIR, "client_stats.csv"))
client_stats_df["timestamp"] = pd.to_datetime(client_stats_df["timestamp"])
client_stats_df["client_ram_%"] = client_stats_df["client_memory_mb"] / AVAILABLE_CLIENT_MEMORY_MB

# # get telemetry data
# telemetry_files = sorted(glob.glob(os.path.join(BASE_OUTPUT_DIR, "telemetry", "*.json")))
# telemetry_datas = []
# for telemetry_file in telemetry_files:
#     with open(telemetry_file, "r") as f:
#         data = json.load(f)
#     for some reason this is incorrect. it only gives the segments of one shard. the other shards dont have a "local" key.
#     segment_count = len(data['result']['collections']['collections'][0]['shards'][0]['local']['segments'])
#     vector_count = data['result']['collections']['collections'][0]['shards'][0]['local']['num_vectors']
#     telemetry_datas.append({"segment_count": segment_count, "vector_count": vector_count})
# telemetry_df = pd.DataFrame(telemetry_datas)


# combine time series data
stats_df = prometheus_df.merge(client_stats_df, on="timestamp", how="outer")
stats_df["filename"] = stats_df["timestamp"].apply(find_experiment_for_timestamp)
stats_df.dropna(subset=["filename"], inplace=True)


# --- AGGREGATION ---

# Define the final columns we want to analyze (using the new converted names)
cols_to_aggregate = ["client_cpu_%", "client_ram_%", "server_ram_%", "server_disk_%", "server_cpu_%"]

final_stats = stats_df.groupby(["filename"])[cols_to_aggregate].agg(["median", "mean", "min", "max"]).reset_index()
final_stats.columns = ["_".join(col).strip("_") for col in final_stats.columns.values]

stats_df_merged = final_stats.merge(benchmark_df, on="filename", how="outer")

# Save


columns_of_interest = ["filename", "max_optimization_threads", "max_indexing_threads", "default_segment_number", "max_segment_size", "indexing_threshold","parallel", "threads", "hnsw_ef", "rps_median", "server_p99", "server_p95"] + [col for col in final_stats.columns if (col.endswith("_max") or col.endswith("_median")) and not col.startswith("rps_")]
stats_df_merged[columns_of_interest].to_csv(os.path.join(BASE_OUTPUT_DIR, "benchmark_summary_hardware.csv"), index=False)
