import glob
import json
import os
import re
from datetime import datetime, timedelta

import numpy as np
import pandas as pd

# ==========================================
# ‚öôÔ∏è CONFIGURATION - EDIT THESE TO MATCH YOUR FILES
# ==========================================
# The CSV generated by the Benchmark script
# BASE_OUTPUT_DIR = "benchmark_results_rw_08"
BASE_OUTPUT_DIR = os.getenv("BASE_OUTPUT_DIR")
BENCHMARK_FOLDER = f"{BASE_OUTPUT_DIR}/raw_benchmark_data"

# Client specs
# January 05, 2026, 15:24:40 (UTC+01:00)
INSTANCE_MAX_BPS = 12.5 * 1_000_000_000  # r6a.2xlarge specs: 8 vCPUs, 64 GiB RAM, Up to 12.5 Gbps Network
AVAILABLE_CLIENT_MEMORY_MB = 64 * 1024

# Server specs
SERVER_NODE_COUNT = 4
# TOTAL_CLUSTER_VCPUS = 64  # 4 nodes * 16 vCPU
# TOTAL_CLUSTER_RAM_GB = 512  # 4 nodes * 128 GB (Adjust if different)
# TOTAL_DISK_CAPACITY = 1024 * 1024 * 1024 * 1024  # 1024GiB

# ==========================================
# üõ†Ô∏è PARSING FUNCTIONS
# ==========================================


def parse_prometheus_data(filepath):
    metrics = {}

    pattern = re.compile(r"^([a-zA-Z0-9_]+)(?:\{(.*)\})?\s+(.+)$")

    with open(filepath, "r") as f:
        contents = f.readlines()

    for line in contents:
        line = line.strip()
        # Skip comments and empty lines
        if not line or line.startswith("#"):
            continue

        match = pattern.match(line)
        if match:
            name = match.group(1)
            label_str = match.group(2)
            value_str = match.group(3)

            # Parse labels into a dict
            labels = {}
            if label_str:
                # Split by comma, but careful with quoted strings containing commas
                # Simple split usually works for standard prom output
                pairs = re.findall(r'([a-zA-Z0-9_]+)="([^"]*)"', label_str)
                labels = {k: v for k, v in pairs}

            try:
                # Handle "Inf" and standard floats
                if "Inf" in value_str:
                    val = float("inf")
                else:
                    val = float(value_str)
            except ValueError:
                continue

            if name not in metrics:
                metrics[name] = []

            metrics[name].append({"labels": labels, "value": val})

    cleaned_metrics = {}
    for key, value in metrics.items():
        value_sum = 0
        for item in value:
            pod = item["labels"].get("pod")
            persistentvolumeclaim = item["labels"].get("persistentvolumeclaim")
            if pod:
                use = pod
            elif persistentvolumeclaim:
                use = persistentvolumeclaim
            else:
                print(f"No pod or persistentvolumeclaim found for {key}")
                continue
            pod_id = use.split("-")[-1]
            new_key = key + "_" + str(pod_id)
            cleaned_metrics[new_key] = item["value"]
            value_sum += item["value"]

        # ----------------------
        # Define prefixes for configuration parameters that should be uniform
        # across nodes and therefore averaged (e.g., vector size, thread limits).
        config_prefixes = "qdrant_collection_config_"

        # Define exact keys that represent pre-calculated averages, timestamps,
        # or logical cluster states that should be averaged.
        keys_to_average_explicit = [
            "collections_total",  # Corrected based on your feedback
            "rest_responses_avg_duration_seconds",
            "rest_responses_max_duration_seconds",
            "rest_responses_min_duration_seconds",
            "kube_pod_status_ready_time",
            # Add any other specific pre-calculated rates/ratios here
        ]

        # Check if the key matches config prefixes or explicit average keys
        should_average = key.startswith(config_prefixes) or key in keys_to_average_explicit

        if should_average:
            # Average: for configs, rates, and logical counts
            cleaned_metrics[key] = value_sum / SERVER_NODE_COUNT
        else:
            # Default to SUM: for counters, capacities, bytes, CPU seconds, and histogram buckets
            cleaned_metrics[key] = value_sum

    return cleaned_metrics


def parse_telemetry_latest(folder_path):
    files = sorted(glob.glob(os.path.join(folder_path, "telemetry", "*.json")))

    total_vectors = 0
    for f in files:
        with open(f, "r") as f:
            data = json.load(f)
            shards = data["result"]["collections"]["collections"][0]["shards"]
            for shard in shards:
                try:
                    total_vectors += shard["local"]["num_vectors"]
                except:
                    continue

    return total_vectors


def compute_benchmark_metrics(file_list):
    """
    Parses a list of Qdrant benchmark JSON files and computes the exact same
    metrics (Min, Avg, Median, P95, P99, Max) as the Rust implementation.
    """

    # Internal helper to match Rust's 'print_stats' logic
    def calc_stats(values, include_percentiles=True):
        if not values:
            return None

        # 1. Sort values (Rust: values.sort_unstable_by)
        values.sort()
        count = len(values)

        # 2. Compute Basic Metrics
        # Rust: values.iter().sum::<f64>() / values.len() as f64
        avg_val = sum(values) / count
        # Rust: values.first()
        min_val = values[0]
        # Rust: values.last()
        max_val = values[-1]

        # Rust: values[(values.len() as f32 * 0.50) as usize]
        # Python int() behaves like Rust cast (truncates decimals)
        p50_idx = int(count * 0.50)
        # Safety clamp not needed for 0.5 unless list is empty, but good practice
        p50_idx = min(p50_idx, count - 1)
        median_val = values[p50_idx]

        stats = {"min": min_val, "avg": avg_val, "median": median_val, "max": max_val}

        if include_percentiles:
            # Rust: values[(values.len() as f32 * 0.95) as usize]
            p95_idx = int(count * 0.95)
            p95_idx = min(p95_idx, count - 1)
            stats["p95"] = values[p95_idx]

            # Rust Loop for P9s (digits=2 for p99)
            # factor = 1.0 - 0.1^2 = 0.99
            # index = ((values.len() as f64 * factor) as usize).min(values.len() - 1)
            p99_idx = int(count * 0.99)
            p99_idx = min(p99_idx, count - 1)
            stats["p99"] = values[p99_idx]

        return stats

    results = []

    for filepath in file_list:
        # extract start and end time from file e.g. "run_P16_T4_EF16_start20251223_085050_end20251223_085057.json"
        start_time = datetime.strptime("_".join(os.path.basename(filepath).split("_start")[1].split("_")[:2]).replace(".json", ""), "%Y%m%d_%H%M%S")
        end_time = datetime.strptime("_".join(os.path.basename(filepath).split("_end")[1].split("_")[:2]).replace(".json", ""), "%Y%m%d_%H%M%S")

        # --- 1. Extract Settings from Filename ---
        filename = os.path.basename(filepath)
        # 'run_Opt"auto"_Idx1_Seg2000000_SegNum200_IndTh200000_P8_T2_EF32_start20260106_115743_end20260106_115900.json'
        # filename = filename.replace("\"auto\"", "auto")
        match = re.search(r"run_Opt(\"auto\"|\d+)_Idx(\d+)_Seg(\d+)_SegNum(\d+)_IndTh(\d+)_P(\d+)_T(\d+)_EF(\d+)", filename)

        if match:
            max_optimization_threads, max_indexing_threads, max_segment_size, default_segment_number, indexing_threshold, parallel, threads, hnsw_ef = match.groups()
        else:
            max_optimization_threads, max_indexing_threads, max_segment_size, default_segment_number, indexing_threshold, parallel, threads, hnsw_ef = (None, None, None, None, None, None, None, None)

        # --- 2. Load JSON Data ---
        with open(filepath, "r") as f:
            data = json.load(f)

        # The Rust struct 'SearcherResults' serializes these keys:
        # server_timings, rps, full_timings
        server_timings_raw = data.get("server_timings", [])
        full_timings_raw = data.get("full_timings", [])
        rps_raw = data.get("rps", [])

        # --- 3. Compute Metrics (Exactly mapping Rust main function calls) ---

        # Call 1: "Search timings" (full_timings) -> percentiles=True
        search_stats = calc_stats(full_timings_raw, include_percentiles=True)

        # Call 2: "Server timings" (server_timings) -> percentiles=True
        server_stats = calc_stats(server_timings_raw, include_percentiles=True)

        # Call 3: "RPS" (rps) -> percentiles=False
        rps_stats = calc_stats(rps_raw, include_percentiles=False)

        # --- 4. Structure Output ---
        entry = {"filename": filename, "start_time": start_time, "end_time": end_time, "max_optimization_threads": max_optimization_threads, "max_indexing_threads": max_indexing_threads, "max_segment_size": max_segment_size, "default_segment_number": default_segment_number, "indexing_threshold": indexing_threshold, "parallel": parallel, "threads": threads, "hnsw_ef": hnsw_ef, **{"search_" + k: v for k, v in search_stats.items()}, **{"server_" + k: v for k, v in server_stats.items()}, **{"rps_" + k: v for k, v in rps_stats.items()}}
        results.append(entry)

    return results


def find_experiment_for_timestamp(timestamp):
    add_seconds = 0
    mask = (benchmark_df["start_time"] <= timestamp + timedelta(seconds=add_seconds)) & (benchmark_df["end_time"] >= timestamp - timedelta(seconds=add_seconds))
    files = benchmark_df[mask]["filename"].values.copy()
    if len(files) == 1:
        return files[0]
    elif len(files) > 1:
        return files[0]  # or handle differently if multiple matches exist
    else:
        return None


# ==========================================
# üìä EXECUTION
# ==========================================

# get benchmark data
files = glob.glob(os.path.join(BENCHMARK_FOLDER, "*.json"))
benchmark_metrics = compute_benchmark_metrics(files)
benchmark_df = pd.DataFrame(benchmark_metrics)
benchmark_df = benchmark_df.sort_values(by=["parallel", "hnsw_ef", "threads", "max_optimization_threads", "max_indexing_threads", "max_segment_size", "default_segment_number", "indexing_threshold"])


# get prometheus data
sys_files = sorted(glob.glob(os.path.join(BASE_OUTPUT_DIR, "raw_sys_metrics", "*.txt")))
prometheus_datas = []
for sys_file in sys_files:
    prometheus_data = parse_prometheus_data(sys_file)
    ts = "_".join(os.path.basename(sys_file).split("_")[-2:]).replace(".txt", "")
    prometheus_data["timestamp"] = datetime.strptime(ts, "%Y%m%d_%H%M%S")
    mem_limit = prometheus_data.get("kube_pod_container_resource_limits")
    prometheus_data["server_ram_%"] = prometheus_data["container_memory_working_set_bytes"] / mem_limit * 100
    disc_limit = prometheus_data.get("kubelet_volume_stats_capacity_bytes")
    prometheus_data["server_disk_%"] = prometheus_data.get("kubelet_volume_stats_used_bytes") / disc_limit * 100
    prometheus_datas.append(prometheus_data)
prometheus_df = pd.DataFrame(prometheus_datas)
prometheus_df = prometheus_df.sort_values(by="timestamp").reset_index(drop=True)
time_delta_seconds = (prometheus_df["timestamp"] - prometheus_df["timestamp"].shift(1)).dt.total_seconds()
cpu_usage_delta = prometheus_df["container_cpu_usage_seconds_total"] - prometheus_df["container_cpu_usage_seconds_total"].shift(1)
cpu_rate_cores = np.where(time_delta_seconds > 0, cpu_usage_delta / time_delta_seconds, 0.0)
raw_limit_nanocores = prometheus_df.get("kube_pod_container_resource_limits", 0)
cpu_limit_standard_cores = raw_limit_nanocores / 1e9
prometheus_df["server_cpu_%"] = np.where(cpu_limit_standard_cores > 0, (cpu_rate_cores / cpu_limit_standard_cores) * 100, 0.0)


# get client stats data
client_stats_df = pd.read_csv(os.path.join(BASE_OUTPUT_DIR, "client_stats.csv"))
client_stats_df["timestamp"] = pd.to_datetime(client_stats_df["timestamp"])
client_stats_df["client_ram_%"] = client_stats_df["client_memory_mb"] / AVAILABLE_CLIENT_MEMORY_MB * 100

client_stats_df.sort_values(by="timestamp", inplace=True)
client_stats_df["time_diff"] = client_stats_df["timestamp"].diff().dt.total_seconds()
client_stats_df["bytes_in_diff"] = client_stats_df["client_net_in_bytes"].diff()
client_stats_df["bytes_out_diff"] = client_stats_df["client_net_out_bytes"].diff()
client_stats_df["bps_in"] = (client_stats_df["bytes_in_diff"] * 8) / client_stats_df["time_diff"]  # Formula: (Bytes Diff * 8 bits/byte) / Time Diff in Seconds
client_stats_df["bps_out"] = (client_stats_df["bytes_out_diff"] * 8) / client_stats_df["time_diff"]
client_stats_df["client_in_%"] = (client_stats_df["bps_in"] / INSTANCE_MAX_BPS) * 100
client_stats_df["client_out_%"] = (client_stats_df["bps_out"] / INSTANCE_MAX_BPS) * 100


# combine time series data
stats_df = prometheus_df.merge(client_stats_df, on="timestamp", how="outer")
stats_df["filename"] = stats_df["timestamp"].apply(find_experiment_for_timestamp)
stats_df.dropna(subset=["filename"], inplace=True)


# --- AGGREGATION ---

# Define the final columns we want to analyze (using the new converted names)
cols_to_aggregate = ["client_cpu_%", "client_ram_%", "client_in_%", "client_out_%", "server_ram_%", "server_disk_%", "server_cpu_%"]
final_stats = stats_df.groupby(["filename"])[cols_to_aggregate].agg(["median", "mean", "min", "max"]).reset_index()
final_stats.columns = ["_".join(col).strip("_") for col in final_stats.columns.values]
stats_df_merged = final_stats.merge(benchmark_df, on="filename", how="outer")

# Save
columns_of_interest = ["filename", "max_optimization_threads", "max_indexing_threads", "default_segment_number", "max_segment_size", "indexing_threshold", "parallel", "threads", "hnsw_ef", "rps_median", "server_p99", "server_p95"] + [col for col in final_stats.columns if (col.endswith("_max") or col.endswith("_median")) and not col.startswith("rps_")]
stats_df_merged[columns_of_interest].to_csv(os.path.join(BASE_OUTPUT_DIR, "benchmark_summary_hardware.csv"), index=False)



















# ----------------------



def flatten_any(d, parent_key="", sep="_"):
    items = []

    if isinstance(d, dict):
        for k, v in d.items():
            new_key = f"{parent_key}{sep}{k}" if parent_key else k
            items.extend(flatten_any(v, new_key, sep=sep).items())
    elif isinstance(d, list):
        items.append((parent_key, str(d)))
    else:
        items.append((parent_key, d))
    return dict(items)


def remove_fields_and_lists(d, fields_to_remove):
    if isinstance(d, dict):
        items = {}
        if any([v in fields_to_remove for v in d.values()]):
            return items
        d = d.copy()
        for k, v in d.items():
            if k not in fields_to_remove:
                items[k] = remove_fields_and_lists(v, fields_to_remove)
        return items
    elif isinstance(d, list):
        return []
    else:
        return d


row_list = []
vector_row_list = []
payload_row_list = []

collections = telemetry_data["result"]["collections"]["collections"]
for collection in collections:
    shards = collection.get("shards", [])
    if isinstance(shards, dict):
        shards = [shards]
    for shard in shards:
        if shard.get("local", {}):
            segments = shard.get("local", {}).get("segments", [])
        else:
            segments = []
        for segment_index, segment in enumerate(segments):

            fields_to_remove = []
            for vector_index_search in segment.get("vector_index_searches", []):
                vector_row = flatten_any(vector_index_search.copy(), parent_key="vector")
                vector_row["collection_id"] = collection["id"]
                vector_row["shard_id"] = shard["id"]
                vector_row["segment_index"] = segment_index
                vector_name = vector_row["vector_index_name"]
                fields_to_remove.append(vector_name)

                flat_info = flatten_any(segment["info"]["vector_data"][vector_name], parent_key="vector")
                for k, v in flat_info.items():
                    if k not in vector_row:
                        vector_row[k] = v
                    else:
                        assert False

                flat_config = flatten_any(segment["config"]["vector_data"][vector_name], parent_key="vector")
                for k, v in flat_config.items():
                    if k not in vector_row:
                        vector_row[k] = v
                    else:
                        assert False

                vector_row_list.append(vector_row)

            for payload_field_index in segment.get("payload_field_indices", []):
                payload_row = flatten_any(payload_field_index.copy(), parent_key="payload")
                payload_row["collection_id"] = collection["id"]
                payload_row["shard_id"] = shard["id"]
                payload_row["segment_index"] = segment_index
                payload_name = payload_row["payload_field_name"]
                fields_to_remove.append(payload_name)

                flat_info = flatten_any(segment["info"]["index_schema"][payload_name], parent_key="payload")
                for k, v in flat_info.items():
                    if k not in payload_row:
                        payload_row[k] = v
                    else:
                        assert False

                payload_row_list.append(payload_row)

            row = {}
            # cluster
            cluster_result_keys = ["id", "app", "cluster", "memory"]
            for k in cluster_result_keys:
                for k, v in flatten_any(telemetry_data["result"][k], parent_key="cluster").items():
                    row[k] = v

            # collection
            for k, v in flatten_any(remove_fields_and_lists(collection.copy(), list(set(fields_to_remove))), parent_key="collection").items():
                row[k] = v
            if row["collection_id"] in telemetry_data["result"]["hardware"]["collection_data"]:
                for k, v in flatten_any(telemetry_data["result"]["hardware"]["collection_data"][row["collection_id"]], parent_key="collection").items():
                    row[k] = v

            # shard
            for k, v in flatten_any(remove_fields_and_lists(shard.copy(), list(set(fields_to_remove))), parent_key="shard").items():
                row[k] = v

            # segment
            row["segment_index"] = segment_index
            for k, v in flatten_any(remove_fields_and_lists(segment.copy(), list(set(fields_to_remove))), parent_key="segment").items():
                row[k] = v

            cleaned_row = {}
            for k, v in row.items():
                if v != "[]":
                    cleaned_row[k] = v
            row_list.append(cleaned_row)


payload_df = pd.DataFrame(payload_row_list)
vector_df = pd.DataFrame(vector_row_list)
segment_df = pd.DataFrame(row_list)


# -------------------------------------------------------------


def get_value(df, value):
    for column in df.columns:
        if column.endswith(value):
            level = column.split("_")[0]
            if level in index_dict:
                index = index_dict[level]
                level_df = df.drop_duplicates(subset=index)
            else:
                level_df = df

            summ = level_df[column].sum()

            if "bytes" in column or "size" in column or "memory" in column:
                # 1. '..._BYTES' columns are already in Bytes.
                # 2. '...RAM_SIZE' or '...DISK_SIZE' (without 'bytes') come from sysinfo as Kilobytes.
                if "bytes" not in column and ("ram_size" in column or "disk_size" in column):
                    # Convert KB to Bytes
                    summ_bytes = summ * 1024
                else:
                    # Already in Bytes
                    summ_bytes = summ

                # Calculate GB and MB based on the normalized byte count
                gb_val = summ_bytes / (1024**3)
                mb_val = summ_bytes / (1024**2)

                if gb_val >= 1:
                    formatted_sum = f"{summ:,}  (~{gb_val:.2f} GB)"
                elif mb_val >= 1:
                    formatted_sum = f"{summ:,}  (~{mb_val:.2f} MB)"
                else:
                    # Optional: Handle very small values (KB)
                    kb_val = summ_bytes / 1024
                    formatted_sum = f"{summ:,}  (~{kb_val:.2f} KB)"
            else:
                formatted_sum = f"{summ:,}"

            print(f"--- source column: {column.upper()}")
            print(f"TOTAL IN CLUSTER: {formatted_sum}")
            if level != "cluster" and summ > 0:
                collection_index = index_dict["collection"]
                grouped_df = level_df.groupby(collection_index)[column].sum().sort_values(ascending=False)
                assert grouped_df.sum() == summ
                print(f"TOP 10 PER {grouped_df[:10]}")
            print("---\n")

            # return column


index_dict = {"cluster": ["cluster"], "collection": ["collection_id"], "shard": ["collection_id", "shard_id"], "segment": ["collection_id", "shard_id", "segment_index"]}

# disc ressource = SHARD_LOCAL_VECTORS_SIZE_BYTES = SHARD_LOCAL_VECTORS_SIZE_BYTES = 34.27 GB
# disc ressource = SHARD_LOCAL_PAYLOADS_SIZE_BYTES = SEGMENT_INFO_PAYLOADS_SIZE_BYTES = 5.93 GB
# Cache (on disc = false) = RAM = disc ressource
# ram max = CLUSTER_SYSTEM_RAM_SIZE = 64 GB


print("=============== General Stats ================")
# total number of collections
total_collections = segment_df[index_dict["collection"]].drop_duplicates().shape[0]
print(f"Total number of collections: {total_collections}")

# total number of shards
total_shards = segment_df[index_dict["shard"]].drop_duplicates().shape[0]
print(f"Total number of shards: {total_shards}")

# total number of segments
total_segments = segment_df[index_dict["segment"]].drop_duplicates().shape[0]
print(f"Total number of segments: {total_segments}")

# number of segments per collection
segments_per_collection = segment_df[index_dict["segment"]].drop_duplicates().groupby(index_dict["collection"])["segment_index"].count().sort_values(ascending=False)[:10]
print("\n=============== Segment Count ================")
print(f"TOP 10 PER {segments_per_collection}")

values = ["active_bytes", "resident_bytes", "disk_size", "ram_size", "features_gpu", "num_points", "num_vectors", "num_deleted_vectors", "num_indexed_vectors", "vectors_size_bytes", "payloads_size_bytes", "disk_usage_bytes", "ram_usage_bytes", "shard_number", "on_disk", "always_ram"]  #
for value in values:
    print(f"\n=============== {value} ================")
    for df in [segment_df, vector_df, payload_df]:
        get_value(df, value)
