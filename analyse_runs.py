import glob
import json
import os
import re
from datetime import datetime, timedelta

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# ==========================================
# ‚öôÔ∏è CONFIGURATION - EDIT THESE TO MATCH YOUR FILES
# ==========================================
# The CSV generated by the Benchmark script
BASE_OUTPUT_DIR = os.getenv("BASE_OUTPUT_DIR")
BENCHMARK_FOLDER = f"{BASE_OUTPUT_DIR}/raw_benchmark_data"


TOTAL_CLUSTER_VCPUS = 64  # 4 nodes * 16 vCPU
TOTAL_CLUSTER_RAM_GB = 512  # 4 nodes * 128 GB (Adjust if different)

# ==========================================
# üõ†Ô∏è PARSING FUNCTIONS
# ==========================================


def parse_prometheus_data(filepath):
    metrics = {}

    pattern = re.compile(r"^([a-zA-Z0-9_]+)(?:\{(.*)\})?\s+(.+)$")

    with open(filepath, "r") as f:
        contents = f.readlines()

    for line in contents:
        line = line.strip()
        # Skip comments and empty lines
        if not line or line.startswith("#"):
            continue

        match = pattern.match(line)
        if match:
            name = match.group(1)
            label_str = match.group(2)
            value_str = match.group(3)

            # Parse labels into a dict
            labels = {}
            if label_str:
                # Split by comma, but careful with quoted strings containing commas
                # Simple split usually works for standard prom output
                pairs = re.findall(r'([a-zA-Z0-9_]+)="([^"]*)"', label_str)
                labels = {k: v for k, v in pairs}

            try:
                # Handle "Inf" and standard floats
                if "Inf" in value_str:
                    val = float("inf")
                else:
                    val = float(value_str)
            except ValueError:
                continue

            if name not in metrics:
                metrics[name] = []

            metrics[name].append({"labels": labels, "value": val})

    cleaned_metrics = {}
    for key, value in metrics.items():
        value_sum = 0
        for item in value:
            pod = item["labels"].get("pod")
            persistentvolumeclaim = item["labels"].get("persistentvolumeclaim")
            if pod:
                use = pod
            elif persistentvolumeclaim:
                use = persistentvolumeclaim
            else:
                print(f"No pod or persistentvolumeclaim found for {key}")
                continue
            pod_id = use.split("-")[-1]
            new_key = key + "_" + pod_id
            cleaned_metrics[new_key] = item["value"]
            value_sum += item["value"]
        cleaned_metrics[key] = value_sum
    return cleaned_metrics


def parse_telemetry_latest(folder_path):
    files = sorted(glob.glob(os.path.join(folder_path, "telemetry", "*.json")))

    total_vectors = 0
    for f in files:
        with open(f, "r") as f:
            data = json.load(f)
            shards = data["result"]["collections"]["collections"][0]["shards"]
            for shard in shards:
                try:
                    total_vectors += shard["local"]["num_vectors"]
                except:
                    continue

    return total_vectors


def compute_benchmark_metrics(file_list):
    """
    Parses a list of Qdrant benchmark JSON files and computes the exact same
    metrics (Min, Avg, Median, P95, P99, Max) as the Rust implementation.
    """

    # Internal helper to match Rust's 'print_stats' logic
    def calc_stats(values, include_percentiles=True):
        if not values:
            return None

        # 1. Sort values (Rust: values.sort_unstable_by)
        values.sort()
        count = len(values)

        # 2. Compute Basic Metrics
        # Rust: values.iter().sum::<f64>() / values.len() as f64
        avg_val = sum(values) / count
        # Rust: values.first()
        min_val = values[0]
        # Rust: values.last()
        max_val = values[-1]

        # Rust: values[(values.len() as f32 * 0.50) as usize]
        # Python int() behaves like Rust cast (truncates decimals)
        p50_idx = int(count * 0.50)
        # Safety clamp not needed for 0.5 unless list is empty, but good practice
        p50_idx = min(p50_idx, count - 1)
        median_val = values[p50_idx]

        stats = {"min": min_val, "avg": avg_val, "median": median_val, "max": max_val}

        if include_percentiles:
            # Rust: values[(values.len() as f32 * 0.95) as usize]
            p95_idx = int(count * 0.95)
            p95_idx = min(p95_idx, count - 1)
            stats["p95"] = values[p95_idx]

            # Rust Loop for P9s (digits=2 for p99)
            # factor = 1.0 - 0.1^2 = 0.99
            # index = ((values.len() as f64 * factor) as usize).min(values.len() - 1)
            p99_idx = int(count * 0.99)
            p99_idx = min(p99_idx, count - 1)
            stats["p99"] = values[p99_idx]

        return stats

    results = []

    for filepath in file_list:
        # extract start and end time from file e.g. "run_P16_T4_EF16_start20251223_085050_end20251223_085057.json"
        start_time = datetime.strptime("_".join(os.path.basename(filepath).split("_start")[1].split("_")[:2]).replace(".json", ""), "%Y%m%d_%H%M%S")
        end_time = datetime.strptime("_".join(os.path.basename(filepath).split("_end")[1].split("_")[:2]).replace(".json", ""), "%Y%m%d_%H%M%S")

        # --- 1. Extract Settings from Filename ---
        filename = os.path.basename(filepath)
        # Matches: run_P{number}_T{number}_EF{number}
        match = re.search(r"run_P(\d+)_T(\d+)_EF(\d+)", filename)

        if match:
            p_val, t_val, ef_val = map(int, match.groups())
        else:
            p_val, t_val, ef_val = (None, None, None)

        # --- 2. Load JSON Data ---
        with open(filepath, "r") as f:
            data = json.load(f)

        # The Rust struct 'SearcherResults' serializes these keys:
        # server_timings, rps, full_timings
        server_timings_raw = data.get("server_timings", [])
        full_timings_raw = data.get("full_timings", [])
        rps_raw = data.get("rps", [])

        # --- 3. Compute Metrics (Exactly mapping Rust main function calls) ---

        # Call 1: "Search timings" (full_timings) -> percentiles=True
        search_stats = calc_stats(full_timings_raw, include_percentiles=True)

        # Call 2: "Server timings" (server_timings) -> percentiles=True
        server_stats = calc_stats(server_timings_raw, include_percentiles=True)

        # Call 3: "RPS" (rps) -> percentiles=False
        rps_stats = calc_stats(rps_raw, include_percentiles=False)

        # --- 4. Structure Output ---
        entry = {"filename": filename, "start_time": start_time, "end_time": end_time, "P": p_val, "T": t_val, "EF": ef_val, **{"search_" + k: v for k, v in search_stats.items()}, **{"server_" + k: v for k, v in server_stats.items()}, **{"rps_" + k: v for k, v in rps_stats.items()}}
        results.append(entry)

    return results


def find_experiment_for_timestamp(timestamp):
    add_seconds = 0
    files = benchmark_df[benchmark_df["start_time"] <= timestamp + timedelta(seconds=add_seconds)][benchmark_df["end_time"] >= timestamp - timedelta(seconds=add_seconds)]["filename"].values.copy()
    if len(files) == 1:
        return files[0]
    elif len(files) > 1:
        # print(f"Multiple experiments found for timestamp {timestamp}")
        return files[0]
    else:
        # print(f"No experiment found for timestamp {timestamp}")
        return None


# ==========================================
# üìä EXECUTION
# ==========================================

# get benchmark data
files = glob.glob(os.path.join(BENCHMARK_FOLDER, "*.json"))
benchmark_metrics = compute_benchmark_metrics(files)
benchmark_df = pd.DataFrame(benchmark_metrics)
benchmark_df = benchmark_df.sort_values(by=["P", "EF", "T"])


# get prometheus data
sys_files = sorted(glob.glob(os.path.join(BASE_OUTPUT_DIR, "raw_sys_metrics", "*.txt")))
prometheus_datas = []
for sys_file in sys_files:
    prometheus_data = parse_prometheus_data(sys_file)
    ts = "_".join(os.path.basename(sys_file).split("_")[-2:]).replace(".txt", "")
    prometheus_data["timestamp"] = datetime.strptime(ts, "%Y%m%d_%H%M%S")
    prometheus_datas.append(prometheus_data)
prometheus_df = pd.DataFrame(prometheus_datas)
prometheus_df = prometheus_df.sort_values(by="timestamp")

# get client stats data
client_stats_df = pd.read_csv(os.path.join(BASE_OUTPUT_DIR, "client_stats.csv"))
client_stats_df["timestamp"] = pd.to_datetime(client_stats_df["timestamp"])


# combine time series data
stats_df = prometheus_df.merge(client_stats_df, on="timestamp", how="outer")
stats_df["filename"] = stats_df["timestamp"].apply(find_experiment_for_timestamp)
stats_df.dropna(subset=["filename"], inplace=True)


# --- UNIT CONVERSION: Gauges ---

# Convert Server RAM from Bytes to GB
stats_df["server_ram_gb"] = stats_df["container_memory_working_set_bytes"] / (1024**3)

# Calculate RAM usage % (Total available is 256GB)
stats_df["server_ram_usage_%"] = (stats_df["server_ram_gb"] / 256.0) * 100

# --- AGGREGATION ---

# Define the final columns we want to analyze (using the new converted names)
cols_to_aggregate = ["client_cpu_%", "client_memory_mb", "server_ram_gb", "server_ram_usage_%"]  # Calculated Rate  # Converted Unit  # Calculated %  # Calculated %  # Calculated Rate

final_stats = stats_df.groupby(["filename"])[cols_to_aggregate].agg(["median", "mean", "min", "max"]).reset_index()
final_stats.columns = ["_".join(col).strip("_") for col in final_stats.columns.values]

stats_df_merged = final_stats.merge(benchmark_df, on="filename", how="outer")

# Save
columns_of_interest = ["filename", "P", "T", "EF", "rps_median", "server_p95", "server_p99"] + [col for col in final_stats.columns if (col.endswith("_max") or col.endswith("_median")) and not col.startswith("rps_")]

stats_df_merged[columns_of_interest].to_csv(os.path.join(BASE_OUTPUT_DIR, "benchmark_summary_hardware.csv"), index=False)

